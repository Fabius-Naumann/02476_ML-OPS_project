import sys
import zipfile
from collections.abc import Iterable
from dataclasses import dataclass
from pathlib import Path

import torch
import typer
from loguru import logger
from torch.utils.data import Dataset
from torchvision import datasets, transforms

from sign_ml import BASE_DIR, FIGURES_DIR, PROCESSED_DIR, RAW_DIR

ZIP_PATH = RAW_DIR / "traffic_signs_merged.zip"
EXTRACT_ROOT = RAW_DIR / "traffic_signs"

TRAIN_FILE = PROCESSED_DIR / "train_preprocessed.pt"
VAL_FILE = PROCESSED_DIR / "val_preprocessed.pt"
TEST_FILE = PROCESSED_DIR / "test_preprocessed.pt"

IMAGE_SIZE = (64, 64)
VAL_SPLIT = 0.2
RANDOM_SEED = 42

PREPROCESS = transforms.Compose(
    [
        transforms.Resize(IMAGE_SIZE),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),
    ]
)
PREPROCESS_OPTION = typer.Option(False, "--preprocess")
SAMPLES_OPTION = typer.Option(9, "--samples", min=1)
OUTPUT_OPTION = typer.Option(FIGURES_DIR / "samples.png", "--output")
REPORT_OPTION = typer.Option(None, "--report")


@dataclass(frozen=True)
class StatisticsArtifacts:
    """Artifacts generated by running dataset statistics."""

    train_samples_path: Path
    val_samples_path: Path
    test_samples_path: Path
    class_distribution_path: Path
    distribution_table: str


def _stratified_train_val_split_indices(
    labels: torch.Tensor,
    *,
    val_fraction: float,
    seed: int,
) -> tuple[torch.Tensor, torch.Tensor]:
    """Create a stratified train/val split.

    This is a lightweight replacement for ``sklearn.model_selection.train_test_split(..., stratify=labels)``.

    Args:
        labels: 1D tensor with class labels.
        val_fraction: Fraction of samples to place in the validation split.
        seed: Random seed for deterministic splitting.

    Returns:
        Tuple of (train_indices, val_indices) as int64 tensors.
    """

    if labels.ndim != 1:
        raise ValueError("labels must be a 1D tensor")
    if not (0.0 < val_fraction < 1.0):
        raise ValueError("val_fraction must be in (0, 1)")

    generator = torch.Generator()
    generator.manual_seed(seed)

    train_indices: list[int] = []
    val_indices: list[int] = []

    unique_labels = torch.unique(labels)
    for label in unique_labels.tolist():
        class_indices = torch.where(labels == label)[0]
        if class_indices.numel() <= 1:
            train_indices.extend(class_indices.tolist())
            continue

        perm = torch.randperm(class_indices.numel(), generator=generator)
        shuffled = class_indices[perm]

        val_count = int(class_indices.numel() * val_fraction)
        val_count = max(1, val_count)
        val_count = min(val_count, int(class_indices.numel() - 1))

        val_indices.extend(shuffled[:val_count].tolist())
        train_indices.extend(shuffled[val_count:].tolist())

    train_tensor = torch.tensor(train_indices, dtype=torch.int64)
    val_tensor = torch.tensor(val_indices, dtype=torch.int64)

    train_tensor = train_tensor[torch.randperm(train_tensor.numel(), generator=generator)]
    val_tensor = val_tensor[torch.randperm(val_tensor.numel(), generator=generator)]
    return train_tensor, val_tensor


class NumericImageFolder(datasets.ImageFolder):
    """ImageFolder that sorts class folders numerically when possible."""

    @staticmethod
    def find_classes(directory: str) -> tuple[list[str], dict[str, int]]:
        classes = [path.name for path in Path(directory).iterdir() if path.is_dir()]
        classes.sort(key=lambda name: int(name) if name.isdigit() else name)
        class_to_idx = {cls_name: idx for idx, cls_name in enumerate(classes)}
        return classes, class_to_idx


def preprocess_data() -> None:
    """Preprocess the raw dataset and store tensors to disk."""

    PROCESSED_DIR.mkdir(parents=True, exist_ok=True)

    if not EXTRACT_ROOT.exists():
        with zipfile.ZipFile(ZIP_PATH, "r") as zip_ref:
            zip_ref.extractall(EXTRACT_ROOT)

    DATA_DIR = EXTRACT_ROOT / "DATA"
    TEST_DIR = EXTRACT_ROOT / "TEST"

    if not DATA_DIR.exists():
        DATA_DIR = EXTRACT_ROOT / "traffic_signs" / "DATA"
        TEST_DIR = EXTRACT_ROOT / "traffic_signs" / "TEST"

    if not (DATA_DIR.exists() and TEST_DIR.exists()):
        raise FileNotFoundError(
            f"Expected traffic sign data directories not found.\n"
            f"Looked for either:\n"
            f"  - {EXTRACT_ROOT / 'DATA'} and {EXTRACT_ROOT / 'TEST'}\n"
            f"  - {EXTRACT_ROOT / 'traffic_signs' / 'DATA'} and "
            f"{EXTRACT_ROOT / 'traffic_signs' / 'TEST'}"
        )
    full_train_ds = NumericImageFolder(DATA_DIR, transform=PREPROCESS)
    test_ds = NumericImageFolder(TEST_DIR, transform=PREPROCESS)

    def to_tensors(dataset: Iterable[tuple[torch.Tensor, int]]) -> tuple[torch.Tensor, torch.Tensor]:
        images: list[torch.Tensor] = []
        labels: list[int] = []
        for img, label in dataset:
            images.append(img)
            labels.append(label)
        return torch.stack(images), torch.tensor(labels)

    train_images, train_labels = to_tensors(full_train_ds)
    test_images, test_labels = to_tensors(test_ds)

    train_idx, val_idx = _stratified_train_val_split_indices(
        train_labels,
        val_fraction=VAL_SPLIT,
        seed=RANDOM_SEED,
    )

    torch.save({"images": train_images[train_idx], "labels": train_labels[train_idx]}, TRAIN_FILE)

    torch.save({"images": train_images[val_idx], "labels": train_labels[val_idx]}, VAL_FILE)

    torch.save({"images": test_images, "labels": test_labels}, TEST_FILE)


class TrafficSignsDataset(Dataset):
    """Dataset backed by preprocessed tensors on disk."""

    def __init__(self, split: str = "train") -> None:
        split = split.lower()
        if split not in {"train", "val", "test"}:
            raise ValueError("split must be train, val, or test")

        file_path = {
            "train": str(TRAIN_FILE),
            "val": str(VAL_FILE),
            "test": str(TEST_FILE),
        }[split]

        if not Path(file_path).exists():
            preprocess_data()

        data = torch.load(file_path, weights_only=True)
        self.images = data["images"]
        self.targets = data["labels"]

    def __len__(self) -> int:
        return self.targets.shape[0]

    def __getitem__(self, idx: int) -> tuple[torch.Tensor, torch.Tensor]:
        return self.images[idx], self.targets[idx]


def run_statistics_viz(samples: int, output: Path) -> StatisticsArtifacts:
    """Run data statistics and visualization."""

    def _summary_stats(targets: torch.Tensor) -> tuple[int, int, float]:
        counts = torch.bincount(targets.to(torch.int64))
        return int(counts.min().item()), int(counts.max().item()), float(counts.float().mean().item())

    def _format_distribution_table(
        train_targets: torch.Tensor,
        val_targets: torch.Tensor,
        test_targets: torch.Tensor,
    ) -> str:
        train_counts = torch.bincount(train_targets.to(torch.int64))
        val_counts = torch.bincount(val_targets.to(torch.int64))
        test_counts = torch.bincount(test_targets.to(torch.int64))

        max_classes = max(train_counts.shape[0], val_counts.shape[0], test_counts.shape[0])
        train_counts = torch.nn.functional.pad(train_counts, (0, max_classes - train_counts.shape[0]))
        val_counts = torch.nn.functional.pad(val_counts, (0, max_classes - val_counts.shape[0]))
        test_counts = torch.nn.functional.pad(test_counts, (0, max_classes - test_counts.shape[0]))

        train_total = int(train_counts.sum().item())
        val_total = int(val_counts.sum().item())
        test_total = int(test_counts.sum().item())

        lines = [
            "| Class | Train Count | Train % | Val Count | Val % | Test Count | Test % |",
            "| ----- | ----------- | ------- | --------- | ----- | ---------- | ------ |",
        ]
        for class_id in range(max_classes):
            train_count = int(train_counts[class_id].item())
            val_count = int(val_counts[class_id].item())
            test_count = int(test_counts[class_id].item())
            train_pct = (train_count / train_total) * 100.0 if train_total else 0.0
            val_pct = (val_count / val_total) * 100.0 if val_total else 0.0
            test_pct = (test_count / test_total) * 100.0 if test_total else 0.0
            lines.append(
                f"| {class_id:>5} | {train_count:>11} | {train_pct:>7.2f}% |"
                f" {val_count:>9} | {val_pct:>5.2f}% | {test_count:>10} | {test_pct:>6.2f}% |"
            )
        lines.append(f"| Total | {train_total:>11} | 100.00% | {val_total:>9} | 100.00% | {test_total:>10} | 100.00% |")
        return "\n".join(lines)

    from sign_ml.visualize import plot_class_distribution, plot_samples

    train_ds = TrafficSignsDataset("train")
    val_ds = TrafficSignsDataset("val")
    test_ds = TrafficSignsDataset("test")

    # plot samples
    train_output = output.with_stem(output.stem + "_train")
    val_output = output.with_stem(output.stem + "_val")
    test_output = output.with_stem(output.stem + "_test")
    plot_samples(train_ds, samples=samples, output_path=train_output)
    plot_samples(val_ds, samples=samples, output_path=val_output)
    plot_samples(test_ds, samples=samples, output_path=test_output)

    class_distribution_output = output.with_stem(output.stem + "_class_distribution")
    num_classes = int(max(train_ds.targets.max().item(), val_ds.targets.max().item(), test_ds.targets.max().item()) + 1)
    train_counts = torch.bincount(train_ds.targets.to(torch.int64), minlength=num_classes)
    val_counts = torch.bincount(val_ds.targets.to(torch.int64), minlength=num_classes)
    test_counts = torch.bincount(test_ds.targets.to(torch.int64), minlength=num_classes)
    plot_class_distribution(
        {"Train": train_counts, "Val": val_counts, "Test": test_counts},
        output_path=class_distribution_output,
    )

    train_min, train_max, train_mean = _summary_stats(train_ds.targets)
    val_min, val_max, val_mean = _summary_stats(val_ds.targets)
    test_min, test_max, test_mean = _summary_stats(test_ds.targets)
    logger.info(
        "Train classes: {}, min: {}, max: {}, mean: {:.1f}",
        num_classes,
        train_min,
        train_max,
        train_mean,
    )
    logger.info(
        "Val classes: {}, min: {}, max: {}, mean: {:.1f}",
        num_classes,
        val_min,
        val_max,
        val_mean,
    )
    logger.info(
        "Test classes: {}, min: {}, max: {}, mean: {:.1f}",
        num_classes,
        test_min,
        test_max,
        test_mean,
    )

    distribution_table = _format_distribution_table(train_ds.targets, val_ds.targets, test_ds.targets)
    return StatisticsArtifacts(
        train_samples_path=train_output,
        val_samples_path=val_output,
        test_samples_path=test_output,
        class_distribution_path=class_distribution_output,
        distribution_table=distribution_table,
    )


def generate_report(samples: int, output: Path, report_path: Path) -> None:
    """Generate a markdown report with dataset statistics and plots."""

    artifacts = run_statistics_viz(samples=samples, output=output)

    def _rel(path: Path) -> str:
        report_dir = report_path.resolve().parent
        target = path.resolve()
        try:
            return target.relative_to(report_dir).as_posix()
        except ValueError:
            try:
                return target.relative_to(BASE_DIR.resolve()).as_posix()
            except ValueError:
                return target.as_posix()

    report_lines = [
        "# Data report",
        "",
        "## Sample images",
        f"![]({_rel(artifacts.train_samples_path)})",
        f"![]({_rel(artifacts.val_samples_path)})",
        f"![]({_rel(artifacts.test_samples_path)})",
        "",
        "## Class distribution",
        f"![]({_rel(artifacts.class_distribution_path)})",
        "",
        "## Class distribution table",
        artifacts.distribution_table,
        "",
    ]

    report_path.write_text("\n".join(report_lines))
    logger.info("Wrote report to {}", report_path)


def benchmark_loading_from_config(
    *,
    experiment: str | None = None,
    split: str = "train",
    distributed: bool = False,
    batch_size: int | None = None,
    num_workers: int | None = None,
    prefetch_factor: int | None = None,
    persistent_workers: bool | None = None,
    pin_memory: bool | None = None,
    multiprocessing_context: str | None = None,
    batches_to_check: int | None = None,
) -> None:
    """Delegate to sign_ml.data_distributed for M29 benchmark.

    Kept here for a stable programmatic entrypoint.
    """

    from sign_ml.data_distributed import (
        benchmark_loading_from_config as _impl,
    )

    _impl(
        experiment=experiment,
        split=split,
        distributed=distributed,
        batch_size=batch_size,
        num_workers=num_workers,
        prefetch_factor=prefetch_factor,
        persistent_workers=persistent_workers,
        pin_memory=pin_memory,
        multiprocessing_context=multiprocessing_context,
        batches_to_check=batches_to_check,
    )


# CLI entry point only available if run as a script
if __name__ == "__main__":

    def main(
        preprocess: bool = PREPROCESS_OPTION,
        samples: int = SAMPLES_OPTION,
        output: Path = OUTPUT_OPTION,
        report: Path | None = REPORT_OPTION,
    ) -> None:
        """Preprocess data or visualize samples.

        Args:
            preprocess: Whether to run preprocessing and exit.
            samples: Number of samples to visualize.
            output: Output image path for the plot.
        """

        if preprocess:
            preprocess_data()
            return

        if report is not None:
            generate_report(samples=samples, output=output, report_path=report)
            return

    # If no CLI args, run benchmark first then generate figures + stats with defaults.
    if len(sys.argv) == 1:
        benchmark_loading_from_config()
        run_statistics_viz(samples=9, output=FIGURES_DIR / "samples.png")
    else:
        typer.run(main)
